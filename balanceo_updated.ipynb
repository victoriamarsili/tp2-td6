{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 2\n",
    "## Tecnología Digital IV\n",
    "### Integrantes: Marsili, Sanson y Rotmistrovsky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías que utilizaremos a lo largo de todo el código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos el conjunto de validación como una submuestra al azar de los 7 conjuntos de datos y generamos 2 nuevos conjuntos uno para Entrenamiento donde estarán los datos que no han sido utilizados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar listas para almacenar los datos de entrenamiento y validación\n",
    "train_data_list = []\n",
    "validation_data_list = []\n",
    "\n",
    "# Número de observaciones por submuestra\n",
    "n_samples = 1_000_000 // 7\n",
    "\n",
    "# Iterar sobre los archivos de datos\n",
    "for i in range(15, 22):\n",
    "    # Cargar los datos\n",
    "    data = pd.read_csv(f\"ctr_{i}.csv\")\n",
    "    \n",
    "    # Verificar que la columna 'Label' existe\n",
    "    if 'Label' not in data.columns:\n",
    "        raise KeyError(f\"La columna 'Label' no se encuentra en el archivo ctr_{i}.csv\")\n",
    "    \n",
    "    # Extraer una submuestra aleatoria de 142,857 observaciones para el conjunto de validación\n",
    "    validation_sample = data.sample(n=n_samples, random_state=42)\n",
    "    validation_data_list.append(validation_sample)\n",
    "    \n",
    "    # Utilizar las observaciones restantes para el conjunto de entrenamiento\n",
    "    train_sample = data.drop(validation_sample.index)\n",
    "    train_data_list.append(train_sample)\n",
    "    \n",
    "    # Liberar memoria\n",
    "    del data, validation_sample, train_sample\n",
    "    gc.collect()\n",
    "\n",
    "# Combinar las submuestras para formar el conjunto de validación\n",
    "validation_data = pd.concat(validation_data_list, ignore_index=True)\n",
    "\n",
    "# Combinar las observaciones restantes para formar el conjunto de entrenamiento\n",
    "train_data = pd.concat(train_data_list, ignore_index=True)\n",
    "\n",
    "# Liberar memoria\n",
    "del validation_data_list, train_data_list\n",
    "gc.collect()\n",
    "\n",
    "# Guardar los nuevos conjuntos de datos en archivos CSV\n",
    "train_data.to_csv(\"new_train_data.csv\", index=False)\n",
    "validation_data.to_csv(\"new_validation_data.csv\", index=False)\n",
    "\n",
    "# Balancear el conjunto de entrenamiento\n",
    "train_class_0 = train_data[train_data['Label'] == 0]\n",
    "train_class_1 = train_data[train_data['Label'] == 1]\n",
    "\n",
    "train_class_0_under = resample(train_class_0, replace=False, n_samples=100000, random_state=42)\n",
    "train_class_1_over = resample(train_class_1, replace=True, n_samples=100000, random_state=42)\n",
    "\n",
    "train_balanced = pd.concat([train_class_0_under, train_class_1_over])\n",
    "train_balanced = train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_balanced.to_csv('train_balanced_new.csv', index=False)\n",
    "\n",
    "# Balancear el conjunto de validación\n",
    "validation_class_0 = validation_data[validation_data['Label'] == 0]\n",
    "validation_class_1 = validation_data[validation_data['Label'] == 1]\n",
    "\n",
    "validation_class_0_under = resample(validation_class_0, replace=False, n_samples=100000, random_state=42)\n",
    "validation_class_1_over = resample(validation_class_1, replace=True, n_samples=100000, random_state=42)\n",
    "\n",
    "validation_balanced = pd.concat([validation_class_0_under, validation_class_1_over])\n",
    "validation_balanced = validation_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "validation_balanced.to_csv('validation_balanced_new.csv', index=False)\n",
    "\n",
    "# Cargar el conjunto de evaluación (sin balanceo)\n",
    "eval_data = pd.read_csv(\"ctr_test.csv\")\n",
    "\n",
    "# Guardar el conjunto de evaluación tal cual\n",
    "eval_data.to_csv('eval_balanced.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos de entrenamiento y validación balanceados\n",
    "train_data = pd.read_csv(\"train_balanced_new.csv\")\n",
    "validation_data = pd.read_csv(\"validation_balanced_new.csv\")\n",
    "\n",
    "# Crear variables adicionales\n",
    "# Interacciones entre niveles de negocio\n",
    "train_data['action_categorical_0_1'] = train_data['action_categorical_0'].astype(str) + '_' + train_data['action_categorical_1'].astype(str)\n",
    "validation_data['action_categorical_0_1'] = validation_data['action_categorical_0'].astype(str) + '_' + validation_data['action_categorical_1'].astype(str)\n",
    "\n",
    "# Transformaciones temporales\n",
    "train_data['auction_hour'] = pd.to_datetime(train_data['auction_time'], unit='s').dt.hour\n",
    "validation_data['auction_hour'] = pd.to_datetime(validation_data['auction_time'], unit='s').dt.hour\n",
    "\n",
    "# Interacciones entre variables numéricas\n",
    "train_data['bidfloor_age_ratio'] = train_data['auction_bidfloor'] / (train_data['auction_age'] + 1)\n",
    "validation_data['bidfloor_age_ratio'] = validation_data['auction_bidfloor'] / (validation_data['auction_age'] + 1)\n",
    "\n",
    "# Variables booleanas combinadas\n",
    "train_data['auction_boolean_sum'] = train_data[['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']].sum(axis=1)\n",
    "validation_data['auction_boolean_sum'] = validation_data[['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']].sum(axis=1)\n",
    "\n",
    "# Características del dispositivo\n",
    "device_type_counts = train_data.groupby('device_id')['device_id_type'].nunique().reset_index()\n",
    "device_type_counts.columns = ['device_id', 'device_type_count']\n",
    "train_data = train_data.merge(device_type_counts, on='device_id', how='left')\n",
    "validation_data = validation_data.merge(device_type_counts, on='device_id', how='left')\n",
    "\n",
    "# Análisis Exploratorio de Datos (EDA) - Conjunto de Entrenamiento\n",
    "# Figura 1: Distribución de la variable objetivo en el conjunto de entrenamiento\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=train_data[\"Label\"])\n",
    "plt.title('Distribución de la Variable Objetivo en el Conjunto de Entrenamiento')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "# Figura 2: Mapa de calor de correlación de características numéricas en el conjunto de entrenamiento\n",
    "plt.figure(figsize=(12, 10))\n",
    "numeric_features_train = train_data.select_dtypes(include='number').columns\n",
    "correlation_matrix_train = train_data[numeric_features_train].corr()\n",
    "sns.heatmap(correlation_matrix_train, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Mapa de Calor de Correlación de Características Numéricas en el Conjunto de Entrenamiento')\n",
    "plt.show()\n",
    "\n",
    "# Análisis Exploratorio de Datos (EDA) - Conjunto de Validación\n",
    "# Figura 3: Distribución de la variable objetivo en el conjunto de validación\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=validation_data[\"Label\"])\n",
    "plt.title('Distribución de la Variable Objetivo en el Conjunto de Validación')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "# Figura 4: Mapa de calor de correlación de características numéricas en el conjunto de validación\n",
    "plt.figure(figsize=(12, 10))\n",
    "numeric_features_val = validation_data.select_dtypes(include='number').columns\n",
    "correlation_matrix_val = validation_data[numeric_features_val].corr()\n",
    "sns.heatmap(correlation_matrix_val, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Mapa de Calor de Correlación de Características Numéricas en el Conjunto de Validación')\n",
    "plt.show()\n",
    "\n",
    "# Separar características y variable objetivo para los datos de entrenamiento y validación\n",
    "y_train = train_data[\"Label\"]\n",
    "X_train = train_data.drop(columns=[\"Label\"])\n",
    "\n",
    "y_val = validation_data[\"Label\"]\n",
    "X_val = validation_data.drop(columns=[\"Label\"])\n",
    "\n",
    "# Liberar memoria\n",
    "del train_data, validation_data\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características y variable objetivo para los datos de entrenamiento y validación\n",
    "y_train = train_data[\"Label\"]\n",
    "X_train = train_data.drop(columns=[\"Label\"])\n",
    "\n",
    "y_val = validation_data[\"Label\"]\n",
    "X_val = validation_data.drop(columns=[\"Label\"])\n",
    "\n",
    "# Liberar memoria\n",
    "del train_data, validation_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos un análisis exploratorio de los datos. Como estos ya se encontraban segmentados en los conjuntos correspondientes realizamos uno para Entrenamiento y otro para Validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuamos con este bloque de código realiza una búsqueda aleatoria de hiperparámetros para un modelo de clasificación XGBoost, con el objetivo de encontrar la mejor combinación de parámetros que maximice la métrica ROC-AUC en un conjunto de validación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns\n",
    "numeric_features = X_train.select_dtypes(include='number').columns\n",
    "categorical_features = X_train.select_dtypes(exclude='number').columns\n",
    "\n",
    "# Preprocessing for numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), numeric_features),\n",
    "        ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define a list of parameters to iterate over\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(100, 550, 50),  # Correcto\n",
    "    'max_depth': np.arange(3, 15),            # Correcto\n",
    "    'min_child_weight': np.arange(1, 10),     # Correcto\n",
    "    'subsample': np.arange(0.5, 1.0, 0.1),   # Corregido: límite superior 1.0\n",
    "    'colsample_bytree': np.arange(0.5, 1.0, 0.1),  # Corregido: límite superior 1.0\n",
    "    'gamma': [0, 0.1, 0.5, 1],                 # Correcto\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3]    # Correcto\n",
    "}\n",
    "\n",
    "# Store the best ROC-AUC score and model\n",
    "best_model = None\n",
    "best_roc_auc = -np.inf\n",
    "\n",
    "# Iterate over parameter combinations\n",
    "for i in range(50):\n",
    "    # Randomly select a combination of parameters\n",
    "    params = {key: np.random.choice(values) for key, values in param_grid.items()}\n",
    "    \n",
    "    # Create and fit the model\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        max_depth=params['max_depth'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        gamma=params['gamma'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        eval_metric='logloss',\n",
    "        random_state=22\n",
    "    )\n",
    "    \n",
    "    # Apply preprocessing and fit the model\n",
    "    pipeline = make_pipeline(preprocessor, model)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calculate ROC-AUC\n",
    "    roc_auc_val = roc_auc_score(y_val, y_val_pred_proba)\n",
    "    print(f\"Iteración {i+1}, ROC-AUC: {roc_auc_val}\")\n",
    "    \n",
    "    # Update the best model if this one is better\n",
    "    if roc_auc_val > best_roc_auc:\n",
    "        best_roc_auc = roc_auc_val\n",
    "        best_model = pipeline\n",
    "        print(f\"Nuevo mejor modelo en la iteración {i+1} con ROC-AUC: {best_roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the best model is found, evaluate on the test set\n",
    "eval_data_num = eval_data.select_dtypes(include='number').drop(columns=[\"id\"])\n",
    "eval_data_cat = eval_data.select_dtypes(exclude='number')\n",
    "eval_data_preprocessed = pd.concat([eval_data_num, eval_data_cat], axis=1)\n",
    "\n",
    "y_preds = best_model.predict_proba(eval_data_preprocessed)[:, 1]\n",
    "\n",
    "# Create submission file\n",
    "submission_ctr_21 = pd.DataFrame({\"id\": eval_data[\"id\"], \"Label\": y_preds})\n",
    "submission_ctr_21[\"id\"] = submission_ctr_21[\"id\"].astype(int)\n",
    "submission_ctr_21.to_csv(\"xgboost_casero.csv\", sep=\",\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TD6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
